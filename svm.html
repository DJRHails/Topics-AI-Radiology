<html>
	<head>
		<title>Support Vector Machines for MRI Brain Image Classification</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<script type="text/javascript"
	  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
	</head>
	<body>

    <div id="wrapper" class="divided">

			<section class="banner onload-content-fade-right style2 fullscreen orient-center content-align-center image-position-center invert">
        <div class="content">
          <h1>Support Vector Machines for MRI Brain Image Classification</h1>
          <p>
						SVMs are a type of supervised machine learning method, useful in classification problems. Hence, they have potential for use in identifying and classifying brain tumours in MRI images.
					</p>
          <ul class="actions vertical">
            <li>
              <a href="#first" class="button big wide smooth-scroll">What are SVMS?</a>
            </li>
          </ul>
        </div>
      </section>

			<!-- What are SVMs-->
				<section class="spotlight style1 orient-left content-align-left image-position-center" id="first">
					<div class="content">
						<h2>What are SVMs?</h2>
						<p>
							Support Vector Machine (SVM) refers to a supervised machine learning algorithm which can be used for both classification or regression challenges.<a id="foot-1-ref" href="#foot-1"><sup>[1]</sup></a>
							<br>
							Consider an n-dimensional space.
							Support vectors refer to the individual co-ordinates in the space, and are the data points collected by individual observation.
							'n' refers to the number of features which each data point would have.
							The support vector machine is the hyper-plane that divides the space and separates the data points into classes.<a id="foot-1-ref" href="#foot-1"><sup>[1]</sup></a>
						</p>
          </div>
					<div class = "image image-with-caption">
            <img src="images/svm/svmexample.png" alt="" />
						<div class="image-caption">
							<p>Example of a very simple SVM<a id="foot-13-ref" href="#foot-13"><sup>[13]</sup></a></p>
						</div>
          </div>
				</section>

				<!-- Goals -->
        <section class="spotlight style1 orient-left content-align-left image-position-center">
          <div class="content">
            <h2>What are the goals of SVM?</h2>
            <ul>The goal of support vector machines is to find the hyper-plane (or function) that:
              <li>separates the two classes (benign tumours and malignant tumours)<a id="foot-1-ref" href="#foot-1"><sup>[1]</sup></li></a>
              <li>maximises the distances between nearest data points (from either class) and itself<a id="foot-1-ref" href="#foot-1"><sup>[1]</sup></li></a>
              <li>correctly classified unseen examples<a id="foot-2-ref" href="#foot-2"><sup>[2]</sup></li></a>
            </ul>
            This hyper-plane is named the optimal separating hyperplane.<a id="foot-2-ref" href="#foot-2"><sup>[2]</sup></a>
          </div>
        </section>

				<!-- Linear -->
        <section class="spotlight style1 orient-right content-align-left image-position-center" >
          <div class="content">
            <h2>How do they work on linearly separable classes?</h2>
            <p>
							If the classes are linearly separable, then there must exist a linear hyper-plane that divides the classes accurately.
							The task therefore its to find this linear optimal separating hyper-plane.
							<br>
							The linear hyper-plane will have the form:
							<br>
							<br>
							$$
								f(x) = w^Tx + b
							$$
							<ul> Where:
								 <li> \(x\) is the input vector (the features)</li>
								 <li> \(w\) is the weight vector (that applies weight to each feature)</li>
								 <li> \(b\) is the bias (translates the hyper-plane)</li>
						  </ul>
							Training the SVM can be done using the Perceptron algorithm which cycles through the training data points, and if the data point isn't classified correctly, then the weights in w are updated in order to accomodate the new data point.
							This repeats until a cycle occurs with no changes.<a id="foot-3-ref" href="#foot-3"><sup>[3]</sup></a>
						</p>
					</div>
          <div class="image image-with-caption">
            <img src="images/svm/svmlinear.png" alt="" />
						<div class="image-caption">
							<p>Example of an SVM trained to classify linearly-separable classes.<a id="foot-11-ref" href="#foot-11"><sup>[11]</sup></a></p>
						</div>
          </div>
        </section>

				<!-- Non-linear -->
        <section class="spotlight style1 orient-left content-align-left image-position-center">
          <div class="content">
            <h2>How do they work on non-linearly separable classes?</h2>
            <p>
							When the classes are non-linearly separable, a kernel is used.
							A kernel is a mathematical function that transforms a low-dimensional input space into a high-dimensional space, allowing classes to become linearly separable.
							Once the input data has been transformed, the SVM can be trained in the same way as linearly-separable classes.<a id="foot-1-ref" href="#foot-1"><sup>[1]</sup></a>
							The choice of kernel is not obvious, and so determining the optimal hyper-plane involves testing with different kernels.
							 </p>
	          </div>
	          <div class="image-with-caption">
	            <img src="images/svm/svmnonlinear.png" alt="" />
							<div class="image-caption">
								<p>Example of an SVM with non-linearly-separable classes.<a id="foot-12-ref" href="#foot-12"><sup>[12]</sup></a></p>
							</div>
	          </div>
	        </section>

					<!-- Kernel -->
					<section class="wrapper style1 align-left">
		        <div class="inner">
		          <h1>What are common kernels?</h1>
		          <p>Kernels often used for MRI brain tumour classification include: </p>
		          <div class="index align-left">

		            <section>
		              <header>
		                <h3>Linear <a id="foot-2-ref" href="#foot-2"><sup>[2]</sub></a></h3>
		              </header>
		              <div class="content">
										$$K(X,X') = X^TX'$$
		              </div>
		            </section>
		            <!--  -->
		            <section>
		              <header>
		                <h3>Quadratic <a id="foot-2-ref" href="#foot-2"><sup>[2]</sub></a></h3>
		              </header>
		              <div class="content">
										$$K(X,X') = (1 + X^TX')^2$$
		              </div>
		            </section>
								<section>
								 <header>
									 <h3>Polynomial <a id="foot-2-ref" href="#foot-2"><sup>[2]</sub></a></h3>
								 </header>
								 <div class="content">
									 $$K(X,X') = (1 + X^TX')^d$$
								 </div>
							 </section>
							 <section>
								<header>
									<h3>Radial Basis Function<a id="foot-14-ref" href="#foot-14"><sup>[14]</sub></a></h3>
								</header>
								<div class="content">
									$$K(X,X') = exp(-\gamma||X-X'||^2)$$
								</div>
							</section>
							<p>Note that there are variations of these.</p>
		          </div>
		        </div>
		      </section>

	      <!-- Features -->
	        <section class="spotlight style1 orient-right content-align-left image-position-center">
	          <div class="content">
	            <h2>What features do we extract from MRI images?</h2>
	            <p>
								One images have been pre-processed, the next step is to extract numerical features.
								The choice of numerical features are important as they will determine the success of any machine learning model.
						    <ul> Features can be grouped into:
									<li>Grey-scale features: these include mean, variance, standard deviation, skewness and kurtosis</li>
									<li>Texture-features: these include entropy, dissimilarity, inverse, energy, contrast and IDM </li>
									<li>Symmetric feature: the exterior symmetry </li>
								</ul>
								Once features are obtained, PCA (Principle Component Analysis) can reduce the number of features needed.
							  This is useful because excessive features increases memory use and computation time.<a id="foot-4-ref" href="#foot-4"><sup>[4]</sup></a>
							</p>
	          </div>
	        </section>

					<section class="wrapper style1 align-left">
		        <div class="inner">
		          <h1>How can we extract these features?</h1>
		          <p>Below are two particular approaches to feature extraction found in the referenced research papers.</p>
		          <div class="index align-left">
		            <!-- GLCM -->
		            <section>
		              <header>
		                <h3>GLCM</h3>
		              </header>
		              <div class="content">
										GLCM is a widely used texture feature extraction technique.<a id="foot-6-ref" href="#foot-6"><sup>[6]</sup></a>
	 	 								GLCM stands for Gray-Level Occurence Matrix, and it is a statistical method of examining texture that considers the spatial relationship of pixels.
	 	 								The GLCM function generates a matrix by finding pairs of pixels with specific values and in specified spatial relationships.
	 	 								Statistical values are then calculated using the matrix.<a id="foot-5-ref" href="#foot-5"><sup>[5]</sup></a>
	 	 								<br>
	 	 								Generated features include contrast, homogeneity, correlation, energy and difference entropy.<a id="foot-6-ref" href="#foot-6"><sup>[6]</sup></a>
		              </div>
		            </section>
		            <!-- Wavelet Transform and SGLDM-->
		            <section>
		              <header>
		                <h3>Wavelet Transform &amp; SGLDM </h3>
		              </header>
		              <div class="content">
										By using 2D wavelet transformation, an MRI brain image is decomposed into four sub-bands.
										The sub-band which has the histogram with maximum variance is selected for further processing.
										This means that you capture the clearest appearance of changes between different textures.
										<br>
										The selected sub-band is processed by the spatial gray level dependence matrix (SGLDM) proposed by R.M.Haralick.
										<ul>By applying the SGLDM, 13 features are computed<a id="foot-7-ref" href="#foot-7"><sup>[7]</sup></a>:
											<li>entropy</li>
											<li>contrast</li>
											<li>information measure of correlation I</li>
											<li>correlation</li>
											<li>inverse difference moment</li>
											<li>sum variance</li>
											<li>variance</li>
											<li>angular second moment</li>
											<li>sum entropy</li>
											<li>difference variance</li>
											<li>difference entropy</li>
											<li>sum average</li>
											<li>information measure of correlation IT</li>
										</ul>
		              </div>
		            </section>
		          </div>
		        </div>
		      </section>

	      <!-- Training an SVM -->
	        <section class="spotlight style1 orient-right content-align-left image-position-center ">
	          <div class="content">
	            <h2>How do we use extracted features to train an SVM?</h2>
	            <p>
								After features have been extracted for each MRI image in the training data, data points need to be constructed (where a data point corresponds to an image).
								Each data point will contain all the values for each feature, and the classification of the image (malignant or benign) as a -1 and 1.<a id="foot-3-ref" href="#foot-3"><sup>[3]</sup></a>
								<br>
								The SVM's hyper-plane is initialised with small random values. Then it is trained using the data points.
							</p>
	          </div>
	        </section>

					<!-- Testing and Measuring Performance -->
					<section class="spotlight style1 orient-left content-align-left image-position-center onscroll-image-fade-in">
						<div class="content">
							<h2>How do we test and measure performance of an SVM?</h2>
							<p>
								Given a trained SVM, testing involves 'plotting' unseen data points (MRI images) in the space and seeing if the SVM's classification of the data point matches the correct classification.
								<br>
								<ul>The options that occur when classifying any data point, for the problem of classifying brain tumours as benign and malignant,are as follows<a id="foot-1-ref" href="#foot-1"><sup>[4]</sup></a>:
									<li>True positive (TP): Malignant brain tumour correctly identified as malignant</li>
									<li>False positive (FP): Benign brain tumour image incorrectly identified as malignant</li>
									<li>True negative (TN): Benign brain tumour image correctly identified as benign</li>
									<li>False negative (FN): Malignant brain tumour incorrectly identified as benign</li>
								</ul>
								<ul>The following statistical measures can be taken:
									<li>Sensitivity: TP/(TP + FN) * 100%</li>
									<li>Specificity: TN/(TN + FP) * 100%</li>
									<li>Accuracy: (TP + TN)/(TP + TN + FP + FN) * 100%</li>
								</ul>
								Sensitivity can be thought of as the proportion of malignant brain tumours correctly identified, and specificity as the proportion of benign brain tumours correctly identified.
								One can argue that sensitivity is more important than specificity, since it is time critical to identify malignant tumours.
							</p>
						</div>
					</section>

	      <!-- Results-->
	        <section class="wrapper style1">
	          <div class="inner">
	            <h2>How do SVMs perform at classifying MRI brain images from MRI images?</h2>
	            <p>
								<ul>Across papers referenced, SVMs have been used in classifiying:
									<li>Normal vs Abnormal MRI brain images</li>
									<li>Benign vs Malignant MRI brain tumour images</li>
									<li>Low grade glioma vs High grade glioma MRI brain tumour images</li>
								</ul>
								All of which involve similar feature extraction and training/testing procedures.
								The results of training SVMs for these purposes are shown below.
								Given that the SVMs appear to achieve a high accuracy in classifying malign and benign brain tumours from MRI images, SVMs may be incredibly useful for assisting doctos with time-critical diagnosis.
							</p>
	          </div>

	          <!-- Gallery -->
	            <div class="gallery style2 big">
	              <article>
	                <a href="images/svm/svmclassification1.png" class="image">
	                  <img src="images/svm/svmclassification1.png" alt="" />
	                </a>
	                <div class="caption">
	                  <h3>Results for SVM's trained to classify normal and abnormal MRI brain images.<a id="foot-4-ref" href="#foot-4"><sup>[4]</sup></h3></a>
	                </div>
	              </article>
	              <article>
	                <a href="images/svm/svmclassification2graph.png" class="image">
	                  <img src="images/svm/svmclassification2graph.png" alt="" />
	                </a>
	                <div class="caption">
	                  <h3>Results for SVM's trained to classify malignant and benign MRI brain tumour images.<a id="foot-6-ref" href="#foot-6"><sup>[6]</sup></a></h3>
	                </div>
	              </article>
	              <article>
	                <a href="images/svm/svmclassification3graph.png" class="image">
	                  <img src="images/svm/svmclassification3graph.png" alt="" />
	                </a>
	                <div class="caption">
	                  <h3>Results for SVM's trained to classify low grade vs high grade glioma MRI brain tumour images .<a id="foot-6-ref" href="#foot-6"><sup>[6]</sup></a></h3>
	                </div>
	              </article>
								<article>
	                <a href="images/svm/svmclassification4.png" class="image">
	                  <img src="images/svm/svmclassification4.png" alt="" />
	                </a>
	                <div class="caption">
	                  <h3>Results for SVM's trained to classify normal and abnormal MRI brain images.<a id="foot-8-ref" href="#foot-8"><sup>[8]</sup></a></h3>
	                </div>
	              </article>
								<article>
	                <a href="images/svm/svmknnclassification.png" class="image">
	                  <img src="images/svm/svmknnclassification.png" alt="" />
	                </a>
	                <div class="caption">
	                  <h3>Results for a SVM-KNN hybrid trained to classify normal and abnormal MRI brain images.<a id="foot-8-ref" href="#foot-8"><sup>[8]</sup></a></h3>
	                </div>
	              </article>
	       			</div>
	        </section>

					<section class="wrapper style1 align-left">
		        <div class="inner">
		          <h1>Other considerations</h1>
		          <p>This page deals with a generalised approach to using SVMs for MRI brain image analysis, however there are other specific considerations:</p>
		          <div class="index align-left">
		            <section>
		              <header>
		                <h3>Overfitting</h3>
		              </header>
		              <div class="content">
										When training an SVM, it is important that the model does not overfit to the training data.
										When an SVM overfits, the error on the training set becomes very small, but becomes large when presented with unseen instances.
										<br>
										One method of avoiding overfitting is K-fold cross validation. The idea is to create a K-fold partition of the whole dataset, repeat K times to use Kâˆ’1 folds for training and a left fold for validation, and finally average the error rates of K experiments.<a id="foot-7-ref" href="#foot-7"><sup>7</sup></h3></a>
		              </div>
		            </section>
								<section>
		              <header>
		                <h3>Choice of kernel values</h3>
		              </header>
		              <div class="content">
										When choosing the RBF as the kernel, there is the choice of \(\gamma\) to be made.
										The \(\gamma\) parameter determines how much a single training example influences the SVM. A lower value means that the influence is greater.
									  <a id="foot-9-ref" href="#foot-9"><sup>9</sup></h3></a>
										<br>
										Similarly, when choosing a polynomial kernel, there is the choice of \(d\). Choosing \(d = 2\) is quite common (and is a quadratic kernel), but choosing a high value of \(d\) will cause K(X, X') to tend to a value of 0 or infinity, thus not proving as effective.<a id="foot-10-ref" href="#foot-10"><sup>[10]</sup></a>
		              </div>
		            </section>
		            <section>
		              <header>
		                <h3>Hybrids</h3>
		              </header>
		              <div class="content">
										One of the results shown above is from a SVM-KNN hybrid<a id="foot-8-ref" href="#foot-8"><sup>[8]</sup></a>. The performance of this hybrid shows that SVM may be best used with other methods to optimise the accuracy of an MRI brain tumour classifier.
		              </div>
		            </section>
		          </div>
		        </div>
		      </section>
					<section id="continue" class="wrapper style1 align-center">
		        <div class="inner">
		          <ul class="actions vertical">
		            <li>
									<a href="pnns.html" class="button big wide smooth-scroll">What's next?</a>
		              <a href="index.html" class="button big wide smooth-scroll">Back to home</a>
		            </li>
		          </ul>
		        </div>
		      </section>

					<section id="footnotes" class="wrapper style1 align-center">
						<div class="inner">
							<h1>Footnotes</h1>
							<ul class="alt align-left">
								<li id="foot-1"><a href="#foot-1-ref" title="return">[1]</a>https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/</li>
								<li id="foot-2"><a href="#foot-2-ref" title="return">[2]</a>Steve R Gunn, Support Vector Machines for Classification and Regression (1998)</li>
								<li id="foot-3"><a href="#foot-3-ref" title="return">[3]</a>http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf</li>
		<li id="foot-4"><a href="#foot-4-ref" title="return">[4]</a>Hari Babu Nandpuru, S. S. Salankar, V. R. Bora, MRI brain cancer classification using Support Vector Machine (2014)</li>
		<li id="foot-5"><a href="#foot-5-ref" title="return">[5]</a>https://uk.mathworks.com/help/images/texture-analysis-using-the-gray-level-co-occurrence-matrix-glcm.html</li>
		<li id="foot-6"><a href="#foot-6-ref" title="return">[6]</a>Vijay Wasule, Poonam Sonar, Classification of brain MRI using SVM and KNN classifier (2017)</li>
		<li id="foot-7"><a href="#foot-7-ref" title="return">[7]</a>Ahmed Kharrat, Mohamed Ben Halima, Mounir Ben Ayed, MRI brain tumor classification using Support Vector Machines and meta-heuristic method(2015)</li>
		<li id="foot-8"><a href="#foot-8-ref" title="return">[8]</a>Ketan Machhale, Hari Babu Nandpuru, Vivek Kapur, MRI brain cancer classification using hybrid classifier (SVM-KNN) (2015)</li>
		<li id="foot-9"><a href="#foot-9-ref" title="return">[9]</a>http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html</li>
		<li id="foot-10"><a href="#foot-10-ref" title="return">[10]</a>https://en.wikipedia.org/wiki/Polynomial_kernel</li>
		<li id="foot-11"><a href="#foot-11-ref" title="return">[11]</a>https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/</li>
		<li id="foot-12"><a href="#foot-12-ref" title="return">[12]</a>https://www.quora.com/What-are-the-advantages-of-support-vector-machines-SVM-compared-with-linear-regression-or-logistic-regression</li>
		<li id="foot-13"><a href="#foot-13-ref" title="return">[13]</a>https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/</li>
		<li id="foot-14"><a href="#foot-14-ref" title="return">[14]</a>https://en.wikipedia.org/wiki/Radial_basis_function_kernel</li>
							</ul>
						</div>
					</section>

	    </div>

	  <!-- Scripts -->
	    <script src="assets/js/jquery.min.js"></script>
	    <script src="assets/js/jquery.scrollex.min.js"></script>
	    <script src="assets/js/jquery.scrolly.min.js"></script>
	    <script src="assets/js/skel.min.js"></script>
	    <script src="assets/js/util.js"></script>
	    <script src="assets/js/main.js"></script>

	</body>
</html>
