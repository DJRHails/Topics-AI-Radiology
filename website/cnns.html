<html>
	<head>
		<title>Convolutional Neural Networks for MRI Brain Image Classification</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	</head>
	<body>

		<div id="wrapper" class="divided">
			<section class="banner onload-content-fade-right style2 fullscreen orient-center content-align-center image-position-center invert">
				<div class="content">
					<h1>Convolutional Neural Networks and their impact on radiology</h1>
					<p></p>
					<ul class="actions vertical">
						<li>
							<a href="#first" class="button big wide smooth-scroll">What is a CNN?</a>
						</li>
					</ul>
				</div>
			</section>

			<section class="spotlight style1 orient-left content-align-left" id="first">
				<div class="content">
					<h2>What is a CNN?</h2>
					<p>
					A CNN is a neural network which classifies images by using kernel convolutions in the hidden layer.
					</p>
					<h3>Quick introduction to neural networks</h3>
					<blockquote cite="https://dl.acm.org/citation.cfm?id=38295">
					A neural network is a system made up of a number of simple, highly interconnected processing elements which process information by their dynamic state response to external inputs.
					<sup><a href="#foot-1">1</a></sup>
					</blockquote>
					<p>
				        Neural nets are usually made up of a series of layers, each of which contains a selection of <i>'nodes'</i> which create an <i>'activation function'</i>.	
					<ul>
					There are typically 3 <i>'types'</i> of layers:
						<li>Input</li>
						<li>Hidden Layers</li>
						<li>Output</li>
					</ul>
					Data (patterns) enter the network via the input layer, are passed through one or more hidden layers and the result is given in the output layer.

					Neural nets enable computers to learn to perform some task by analysing training samples. Often this is done using data which has been labelled so corrections in the model can be made. This is called supervised learning.
					</p>
					<h3>Convolutions</h3>
					<blockquote>
						A convolution is a mathematical operation on two functions to produce a third function which is seen as a modification of one of the first functions, giving the integral of the pointwise multiplication of the two functions as a function of the amount that one of the original functions is translated.
						<sup><a href="#foot-2">2</a></sup></blockquote>
					<p>
					Convolution is defined as:
					$$ (f * g )(t) = \int_{0}^{t} f(\tau)\, g(t - \tau)\, d\tau\ \mathrm{for} \ \ f, g : [0, \infty) \to \mathbb{R} $$
					where \( (f * g )(t) \) represents the convolution of \(f\) on \(g\). For discrete functions this is analogous to:
					$$ (f * g)[n]\ \stackrel{\mathrm{def}}{=}\ \sum_{m=-\infty}^\infty f[m]\, g[n - m] $$
					</p>
					<h3>CNN</h3>
					<p>So, putting those together, a CNN will try to find features in a new image by passing performing a convolution of the feature matrix over the image. This will result in a new map of where in the image the feature is found.</p>
					<p>Each layer will use various nodes, which are really feature maps, from the previous layer to compute more complex features.</p>
				
					<p>Typically, in the hidden layers, there are 3 types of layers - <b>convolutional layers</b>, <b>pooling layers</b>, and <b>activation layers</b>.</p>
					
					<ul>
						<li>Convolutional Layers - this is where the convolutions happen.</li>
						<li>Pooling layers - used to take large images and make them smaller while preserving the most important information.</li>
						<li>Activation layers - a layer which performs an activation process to ensure the feature does not get lost at deeper layers. Often this is ReLU (Rectified Linear Units), which keeps positive values unchanged and negatives are set to 0.
							<sup>
								<a href="#foot-3">3</a>
							</sup>
							
						</li>
					</ul>
					<p>One final type of layer is a fully connected layer. A CNN will have at least one of these at the end and these are responsible for converting the output of the convolution, pooling and activation layers into votes for a particular class.</p>
					
					
					
				</div>
			</section>
			<section class="spotlight style1 orient-left content-align-left" id="first">
				<div class="content style2">
					<h2>Inside a CNN</h2>
					<article>
						<p>
						As mentioned above, a CNN consists of an input layer, a series of hidden layers, one or more fully connected layers and an output layer.
						</p>
						<figure>
						<img src="images/cnn-layers.png" width="100%" alt="" />
						<figcaption>Looking inside the hidden layers of a CNN<sup><a href=#"foot-3">3</a></sup></figcaption>
						</figure>
					</article>
					<article>
						<p>
						</p>
					</article>
				</div>
			</section>

			<section class="spotlight style1 orient-left content-align-left image-position-center">
				<div class="content">
					<h2>How does a CNN performs classification?</h2>
					<ul>The goal of support vector machines is to find the hyper-plane (or function) that:
						<li>separates the two classes (benign tumours and malignant tumours)<sup>1</sup></li>
						<li>maximises the distances between nearest data points (from either class) and itself<sup>1</sup></li>
						<li>correctly classified unseen examples<sup>2</sup></li>
					</ul>
					This hyper-plane is named the optimal separating hyperplane.<sup>2</sup>
				</div>
			</section>

			<section class="spotlight style1 orient-right content-align-left image-position-center" >
				<div class="content">
					<h2>Training</h2>
					<p>
					If the classes are linearly separable, then there must exist a linear hyper-plane that divides the classes accurately.
					The task therefore its to find this linear optimal separating hyper-plane.
					<br>
					The linear hyper-plane will have the form:
					<br>
					<br>
					<ul> Where:
						<li><math>x</math> is the input vector (the features)</li>
						<li><math>w</math> is the weight vector (that applies weight to each feature)</li>
						<li><math>b</math> is the bias (translates the hyper-plane)</li>
					</ul>
					Training the SVM can be done using the Perceptron algorithm which cycles through the training data points, and if the data point isn't classified correctly, then the weights in w are updated in order to accomodate the new data point. This repeats until a cycle occurs with no changes.<sup>3</sup>
					</p>
				</div>
				<div class="image">
					<img src="images/svm2.png" alt="" />
				</div>
			</section>

			<section class="spotlight style1 orient-left content-align-left image-position-center">
				<div class="content">
					<h2>Using CNN's for multiple object classification</h2>
					<p>
					When the classes are non-linearly separable, a kernel is used.
					A kernel is a mathematical function that transforms a low-dimensional input space into a high-dimensional space, allowing classes to become linearly separable.
					Once the input data has been transformed, the SVM can be trained in the same way as linearly-separable classes.<sup>1</sup>
					<br>
					Kernels often used for MRI brain tumour classification include <sup>2</sub>:
					<ul>
					</ul>
					The choice of kernel is not obvious, and so determining the optimal hyper-plane involves testing with different kernels.
					</p>
				</div>
				<div class="image">
					<img src="images/svm1.png" alt="" />
				</div>
			</section>

			<!-- Four -->
			<section class="spotlight style1 orient-right content-align-left image-position-center">
				<div class="content">
					<h2>Impact on medical image analysis</h2>
					<p>
					One images have been pre-processed, the next step is to extract numerical features.
					The choice of numerical features are important as they will determine the success of any machine learning model.
					<ul> Features can be grouped into:
						<li>Grey-scale features: these include mean, variance, standard deviation, skewness and kurtosis</li>
						<li>Texture-features: these include entropy, dissimilarity, inverse, energy, contrast and IDM </li>
						<li>Symmetric feature: the exterior symmetry </li>
					</ul>
					Once features are obtained, PCA (Principle Component Analysis) can reduce the number of features needed.
					This is useful because excessive features increases memory use and computation time.<sup>4</sup>
					</p>
				</div>
			</section>

			<!-- Four -->
			<section class="spotlight style1 orient-right content-align-left image-position-center">
				<div class="content">
					<h2>Fine tuning or full training</h2>
					<p>
					One images have been pre-processed, the next step is to extract numerical features.
					The choice of numerical features are important as they will determine the success of any machine learning model.
					<ul> Features can be grouped into:
						<li>Grey-scale features: these include mean, variance, standard deviation, skewness and kurtosis</li>
						<li>Texture-features: these include entropy, dissimilarity, inverse, energy, contrast and IDM </li>
						<li>Symmetric feature: the exterior symmetry </li>
					</ul>
					Once features are obtained, PCA (Principle Component Analysis) can reduce the number of features needed.
					This is useful because excessive features increases memory use and computation time.<sup>4</sup>
					</p>
				</div>
			</section>



			<section id="continue" class="wrapper style1 align-center">
				<div class="inner">
					<ul class="actions vertical">
						<li>
							<a href="index.html" class="button big wide smooth-scroll">Back to home</a>
						</li>
					</ul>
				</div>
			</section>

			<section id="footnotes" class="wrapper style1 align-center">
				<div class="inner">
					<h1>Footnotes</h1>
					<ul class="alt align-left">
						<li id="foot-1"><a href="#foot-1-ref" title="return">[1]</a>https://dl.acm.org/citation.cfm?id=38295</li>
						<li id="foot-2"><a href="#foot-2-ref" title="return">[2]</a>https://en.wikipedia.org/wiki/Convolution</li>
						<li id="foot-3"><a href="#foot-3-ref" title="return">[3]</a>https://brohrer.github.io/how_convolutional_neural_networks_work.html</li>
						<li id="foot-5"><a href="#foot-5-ref" title="return">[5]</a>Odland A, Server A, Saxhaug C, Breivik B, Groote R, Vardal J, Larsson C, Bjørnerud A. Volumetric glioma quantification: comparison of manual and semi-automatic tumor segmentation for the quantification of tumor growth. Acta Radiol. 2015;56:1396–1403. doi: 10.1177/0284185114554822. [PubMed] [Cross Ref]]</li>
						<li id="foot-6"><a href="#foot-6-ref" title="return">[6]</a>Aslian H, Sadeghi M, Mahdavi SR, Babapour Mofrad F, Astarakee M, Khaledi N, Fadavi P. Magnetic resonance imaging-based target volume delineation in radiation therapy treatment planning for brain tumors using localized region-based active contour. Int J Radiat Oncol Biol Phys. 2013;87:195–201. doi: 10.1016/j.ijrobp.2013.04.049. </li>
						<li id="foot-7"><a href="#foot-7-ref" title="return">[7]</a>BRATS dataset</li>
						<li id="foot-8"><a href="#foot-8-ref" title="return">[8]</a>Radiology Masterclass https://www.radiologymasterclass.co.uk/tutorials/mri/mri_sequences</li>
						<li id="foot-9"><a href="#foot-9-ref" title="return">[9]</a> http://ieeexplore.ieee.org/document/6975210/</li>
					</ul>
				</div>
			</section>

		</div>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.scrollex.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/skel.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

	</body>
</html>
