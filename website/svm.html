<html>
	<head>
		<title>Support Vector Machines for MRI Brain Image Classification</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<script type="text/javascript"
	  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
	</head>
	<body>

    <div id="wrapper" class="divided">

			<section class="banner onload-content-fade-right style2 fullscreen orient-center content-align-center image-position-center invert">
        <div class="content">
          <h1>Support Vector Machines for MRI Brain Image Classification</h1>
          <p>
						SVMs are a type of supervised machine learning method which are effective for classification problems.
						Hence, they have potential for use in automating the identification and classification of brain tumours in MRI images.
					</p>
          <ul class="actions vertical">
            <li>
              <a href="#first" class="button big wide smooth-scroll">What are SVMS?</a>
            </li>
          </ul>
        </div>
      </section>

			<!-- What are SVMs-->
				<section class="spotlight style1 orient-left content-align-left image-position-center" id="first">
					<div class="content">
						<h2>What are SVMs?</h2>
						<p>
							Support Vector Machine (SVM) refers to a supervised machine learning algorithm which can be used for both classification or regression challenges.<a id="foot-1-ref" href="#foot-1"><sup>[1]</sup></a>
							<br>
							SVM performs classification by finding the hyperplane that maximises the margin between two classes.
							In the problem of classifying MRI brain tumour images the two classes can be, for instance, malignant and benign tumour images.
							Support vectors define the hyper-plane.<a id="foot-15-ref" href="#foot-15"><sup>[15]</sup></a>
						</p>
          </div>
					<div class = "image image-with-caption">
            <img src="images/svm/svmexample.png" alt="" />
						<div class="image-caption">
							<p>Example of a very simple SVM<a id="foot-13-ref" href="#foot-13"><sup>[13]</sup></a></p>
						</div>
          </div>
				</section>

				<!-- Goals -->
        <section class="spotlight style1 orient-left content-align-left image-position-center">
          <div class="content">
            <h2>What are the goals of SVM?</h2>
            <ul>The goal of support vector machines is to find the hyper-plane (or function) that:
              <li>separates the two classes (benign tumours and malignant tumours)<a id="foot-1-ref" href="#foot-1"><sup>[1]</sup></li></a>
              <li>maximises the distances between nearest data points (from either class) and itself<a id="foot-1-ref" href="#foot-1"><sup>[1]</sup></li></a>
              <li>correctly classified unseen examples<a id="foot-2-ref" href="#foot-2"><sup>[2]</sup></li></a>
            </ul>
            This hyper-plane is named the optimal separating hyperplane.<a id="foot-2-ref" href="#foot-2"><sup>[2]</sup></a>
          </div>
        </section>

				<!-- Linear -->
        <section class="spotlight style1 orient-right content-align-left image-position-center" >
          <div class="content">
            <h2>How do they work on linearly separable classes?</h2>
            <p>
							If the classes are linearly separable then there must exist a linear hyper-plane that divides the classes accurately.
							The task therefore its to find this linear optimal separating hyper-plane.
							<br>
							The linear hyper-plane can have the primal form:
							$$
								f(x) = w^Tx + b
							$$
							There is also the dual form:
							$$
								f(x) = \sum_{i}^{N}\alpha_{i}y_{i}(x_{i}^Tx) + b
							$$
							<ul>Where:
								<li>\(b\) is the bias</li>
								<li>\(w\) is a (primal) weight vector</li>
								<li>\(\alpha\) is a (dual) weight vector</li>
								<li>\(y_{i} \in \{-1, +1\}\)</li>
								<li>\(x_{i}\) is a support vector</li>
							</ul>
							<a id="foot-17-ref" href="#foot-17"><sup>[17]</sup></a>
							</a>
						</p>
					</div>
          <div class="image image-with-caption">
            <img src="images/svm/svmlinear.png" alt="" />
						<div class="image-caption">
							<p>Example of an SVM trained to classify linearly-separable classes.<a id="foot-11-ref" href="#foot-11"><sup>[11]</sup></a></p>
						</div>
          </div>
        </section>

				<!-- Non-linear -->
        <section class="spotlight style1 orient-left content-align-left image-position-center">
          <div class="content">
            <h2>How do they work on non-linearly separable classes?</h2>
            <p>
							When the classes are non-linearly separable it makes more sense to use the dual form of classifier.
							However, a kernel is also needed.
							A kernel is a mathematical function that transforms a low-dimensional input space into a high-dimensional space, allowing the modified classes to become linearly separable<a id="foot-1-ref" href="#foot-1"><sup>[1]</sup></a>.
							<br>
							For a kernel \(K(x_{i}, x)\), the dual classifier becomes<a id="foot-17-ref" href="#foot-17"><sup>[17]</sup></a>:
							$$
								f(x) = \sum_{i}^{N}\alpha_{i}y_{i}K(x_{i}, x) + b
							$$
						</p>
	          </div>
	          <div class="image-with-caption">
	            <img src="images/svm/svmnonlinear.png" alt="" />
							<div class="image-caption">
								<p>Example of an SVM with non-linearly-separable classes.<a id="foot-12-ref" href="#foot-12"><sup>[12]</sup></a></p>
							</div>
	          </div>
	        </section>

					<!-- Kernel -->
					<section class="wrapper style1 align-left">
		        <div class="inner">
		          <h1>What are common kernels?</h1>
		          <p>
								Some examples of kernels are shown below:
							</p>
		          <div class="index align-left">

		            <section>
		              <header>
		                <h3>Linear <a id="foot-2-ref" href="#foot-2"><sup>[2]</sub></a></h3>
		              </header>
		              <div class="content">
										$$K(x_{i}, x) = x_{i}^Tx$$
		              </div>
		            </section>
		            <!--  -->
		            <section>
		              <header>
		                <h3>Quadratic <a id="foot-2-ref" href="#foot-2"><sup>[2]</sub></a></h3>
		              </header>
		              <div class="content">
										$$K(x_{i},x) = (1 + x_{i}^Tx)^2$$
		              </div>
		            </section>
								<section>
								 <header>
									 <h3>Polynomial <a id="foot-2-ref" href="#foot-2"><sup>[2]</sub></a></h3>
								 </header>
								 <div class="content">
									 $$K(x_{i},x) = (1 + x_{i}^Tx)^d$$
								 </div>
							 </section>
							 <section>
								<header>
									<h3>Radial Basis Function<a id="foot-14-ref" href="#foot-14"><sup>[14]</sub></a></h3>
								</header>
								<div class="content">
									$$K(x_{i},x) = exp(-\gamma||x_{i}-x||^2)$$
								</div>
							</section>
							<p>Note that there are variations of these.</p>
		          </div>
		        </div>
		      </section>

	      <!-- Features -->
	        <section class="spotlight style1 orient-right content-align-left image-position-center">
	          <div class="content">
	            <h2>What features do we extract from MRI images?</h2>
	            <p>
								Once images have been pre-processed the next step is to extract numerical features.
								The choice of numerical features are important as they will determine the success of any machine learning model.
						    <ul> Features can be grouped into:
									<li>Grey-scale features: these include mean, variance, standard deviation, skewness and kurtosis</li>
									<li>Texture-features: these include entropy, dissimilarity, inverse, energy, contrast and IDM </li>
									<li>Symmetric feature: the exterior symmetry </li>
								</ul>
								Once features are obtained PCA (Principle Component Analysis) can reduce the number of features needed.
							  This is useful because excessive features increase memory use and computation time.<a id="foot-4-ref" href="#foot-4"><sup>[4]</sup></a>
							</p>
	          </div>
	        </section>

					<section class="wrapper style1 align-left">
		        <div class="inner">
		          <h1>How can we extract these features?</h1>
		          <p>Below are two particular approaches to feature extraction found in the referenced research papers.</p>
		          <div class="index align-left">
		            <!-- GLCM -->
		            <section>
		              <header>
		                <h3>GLCM</h3>
		              </header>
		              <div class="content">
										GLCM is a widely used texture feature extraction technique.<a id="foot-6-ref" href="#foot-6"><sup>[6]</sup></a>
	 	 								GLCM stands for Grey-Level Occurence Matrix and it is a statistical method of examining texture that considers the spatial relationship of pixels.
	 	 								The GLCM function generates a matrix by finding pairs of pixels with specific values and in specified spatial relationships.
	 	 								Statistical values are then calculated using the matrix.<a id="foot-5-ref" href="#foot-5"><sup>[5]</sup></a>
	 	 								<br>
	 	 								Generated features include contrast, homogeneity, correlation, energy, and difference entropy.<a id="foot-6-ref" href="#foot-6"><sup>[6]</sup></a>
		              </div>
		            </section>
		            <!-- Wavelet Transform and SGLDM-->
		            <section>
		              <header>
		                <h3>Wavelet Transform &amp; SGLDM </h3>
		              </header>
		              <div class="content">
										By using 2D wavelet transformation an MRI brain image is decomposed into four sub-bands.
										The sub-band which has the histogram with maximum variance is selected for further processing.
										This means that you capture the clearest appearance of changes between different textures.
										<br>
										The selected sub-band is processed by the spatial Grey-Level Dependence Matrix (SGLDM) proposed by R.M.Haralick.
										<ul>By applying the SGLDM, 13 features are computed<a id="foot-7-ref" href="#foot-7"><sup>[7]</sup></a>:
											<li>entropy</li>
											<li>contrast</li>
											<li>information measure of correlation I</li>
											<li>correlation</li>
											<li>inverse difference moment</li>
											<li>sum variance</li>
											<li>variance</li>
											<li>angular second moment</li>
											<li>sum entropy</li>
											<li>difference variance</li>
											<li>difference entropy</li>
											<li>sum average</li>
											<li>information measure of correlation IT</li>
										</ul>
		              </div>
		            </section>
		          </div>
		        </div>
		      </section>

	      <!-- Training an SVM -->
	        <section class="spotlight style1 orient-right content-align-left image-position-center ">
	          <div class="content">
	            <h2>How do we use extracted features to train an SVM?</h2>
	            <p>
								After features have been extracted for each MRI image in the training data, data points need to be constructed (where a data point corresponds to an image).
								Each data point will contain all the values for each feature and the classification of the image (malignant or benign) as a -1 and +1.<a id="foot-3-ref" href="#foot-3"><sup>[3]</sup></a>
								<br>
								The SVM is then trained using each data point. For the dual classifier training involves learning the values in \(\alpha\) to determine the hyper-plane (and which data points are its support vectors).
								<br>
								One modern algorithm used for training a dual classifier is Co-ordinate Descent.
								For each \(i \in \{1 .. n\}\), iteratively, the co-efficient \(\alpha_{i}\) is adjusted in the direction that reduces the error.
								Then, \(\alpha\) is projected onto the nearest vector of co-efficients that satisfies the given constraints (typically Euclidean distances).
								This is repeated until near optimal coefficients are obtained. <a id="foot-16-ref" href="#foot-16"><sup>[16]</sup></a>
							</p>
	          </div>
	        </section>

					<!-- Testing and Measuring Performance -->
					<section class="spotlight style1 orient-left content-align-left image-position-center onscroll-image-fade-in">
						<div class="content">
							<h2>How do we test and measure performance of an SVM?</h2>
							<p>
								Given a trained SVM testing involves 'plotting' unseen data points (MRI images) in the space, and seeing if the SVM's classification of the data point matches the correct classification.
								<br>
								<ul>The options when classifying any data point, for the problem of classifying brain tumours as benign and malignant, are as follows<a id="foot-1-ref" href="#foot-1"><sup>[4]</sup></a>:
									<li>True positive (TP): Malignant brain tumour correctly identified as malignant</li>
									<li>False positive (FP): Benign brain tumour image incorrectly identified as malignant</li>
									<li>True negative (TN): Benign brain tumour image correctly identified as benign</li>
									<li>False negative (FN): Malignant brain tumour incorrectly identified as benign</li>
								</ul>
								<ul>The following statistical measures can be taken:
									<li>Sensitivity: TP/(TP + FN) * 100%</li>
									<li>Specificity: TN/(TN + FP) * 100%</li>
									<li>Accuracy: (TP + TN)/(TP + TN + FP + FN) * 100%</li>
								</ul>
								Sensitivity can be thought of as the proportion of malignant brain tumours correctly identified, and specificity as the proportion of benign brain tumours correctly identified.
								One can argue that sensitivity is more important than specificity since it is time critical to identify malignant tumours.
							</p>
						</div>
					</section>

	      <!-- Results-->
	        <section class="wrapper style1">
	          <div class="inner">
	            <h2>How do SVMs perform at classifying MRI brain images from MRI images?</h2>
	            <p>
								<ul>Across papers referenced, SVMs have been used in classifiying:
									<li>Normal vs Abnormal MRI brain images</li>
									<li>Benign vs Malignant MRI brain tumour images</li>
									<li>Low grade glioma vs High grade glioma MRI brain tumour images</li>
								</ul>
								All of which involve similar feature extraction and training/testing procedures.
								The results of training SVMs for these purposes are shown below.
								Given that the SVMs appear to achieve a high accuracy in classifying malignant and benign brain tumours from MRI images, SVMs may be incredibly useful for assisting doctos with time-critical diagnosis.
							</p>
	          </div>

	          <!-- Gallery -->
	            <div class="gallery style2 big">
	              <article>
	                <a href="images/svm/svmclassification1.png" class="image">
	                  <img src="images/svm/svmclassification1.png" alt="" />
	                </a>
	                <div class="caption">
	                  <h3>Results for SVM's trained to classify normal and abnormal MRI brain images.<a id="foot-4-ref" href="#foot-4"><sup>[4]</sup></a></h3>
	                </div>
	              </article>
	              <article>
	                <a href="images/svm/svmclassification2graph.png" class="image">
	                  <img src="images/svm/svmclassification2graph.png" alt="" />
	                </a>
	                <div class="caption">
	                  <h3>Results for SVM's trained to classify malignant and benign MRI brain tumour images.<a id="foot-6-ref" href="#foot-6"><sup>[6]</sup></a></h3>
	                </div>
	              </article>
	              <article>
	                <a href="images/svm/svmclassification3graph.png" class="image">
	                  <img src="images/svm/svmclassification3graph.png" alt="" />
	                </a>
	                <div class="caption">
	                  <h3>Results for SVM's trained to classify low grade vs high grade glioma MRI brain tumour images .<a id="foot-6-ref" href="#foot-6"><sup>[6]</sup></a></h3>
	                </div>
	              </article>
								<article>
	                <a href="images/svm/svmclassification4.png" class="image">
	                  <img src="images/svm/svmclassification4.png" alt="" />
	                </a>
	                <div class="caption">
	                  <h3>Results for SVM's trained to classify normal and abnormal MRI brain images.<a id="foot-8-ref" href="#foot-8"><sup>[8]</sup></a></h3>
	                </div>
	              </article>
								<article>
	                <a href="images/svm/svmknnclassification.png" class="image">
	                  <img src="images/svm/svmknnclassification.png" alt="" />
	                </a>
	                <div class="caption">
	                  <h3>Results for a SVM-KNN hybrid trained to classify normal and abnormal MRI brain images.<a id="foot-8-ref" href="#foot-8"><sup>[8]</sup></a></h3>
	                </div>
	              </article>
	       			</div>
	        </section>

					<section class="wrapper style1 align-left">
		        <div class="inner">
		          <h1>Other considerations</h1>
		          <p>This page deals with a generalised approach to using SVMs for MRI brain image analysis, however, there are other specific considerations:</p>
		          <div class="index align-left">
		            <section>
		              <header>
		                <h3>Overfitting</h3>
		              </header>
		              <div class="content">
										When training an SVM it is important that the model does not overfit to the training data.
										When an SVM overfits the error on the training set becomes very small, but becomes large when presented with unseen instances.
										<br>
										One method of avoiding overfitting is K-fold cross validation. The idea is to create a K-fold partition of the whole dataset, repeat K times to use K−1 folds for training and a left fold for validation, and finally average the error rates of K experiments.<a id="foot-7-ref" href="#foot-7"><sup>7</sup></a></h3>
		              </div>
		            </section>
								<section>
		              <header>
		                <h3>Choice of kernel values</h3>
		              </header>
		              <div class="content">
										When choosing the RBF as the kernel there is the choice of \(\gamma\) to be made.
										The \(\gamma\) parameter determines how much a single training example influences the SVM. A lower value means that the influence is greater.
									  <a id="foot-9-ref" href="#foot-9"><sup>9</sup></a>
										<br>
										Similarly, when choosing a polynomial kernel there is the choice of \(d\). Choosing \(d = 2\) is quite common (and is a quadratic kernel), but choosing a high value of \(d\) will cause \(K(x_{i}, x)\) to tend to a value of 0 or infinity, thus, not proving as effective.<a id="foot-10-ref" href="#foot-10"><sup>[10]</sup></a>
		              </div>
		            </section>
		            <section>
		              <header>
		                <h3>Hybrids</h3>
		              </header>
		              <div class="content">
										One of the results shown above is from a SVM-KNN hybrid<a id="foot-8-ref" href="#foot-8"><sup>[8]</sup></a>. The performance of this hybrid shows that SVM may be best used with other methods to optimise the accuracy of an MRI brain tumour classifier.
		              </div>
		            </section>
		          </div>
		        </div>
		      </section>
					<section id="continue" class="wrapper style1 align-center">
		        <div class="inner">
		          <ul class="actions vertical">
		            <li>
									<a href="pnns.html" class="button big wide smooth-scroll">What's next?</a>
		              <a href="index.html" class="button big wide smooth-scroll">Back to home</a>
		            </li>
		          </ul>
		        </div>
		      </section>

					<section id="footnotes" class="wrapper style1 align-center">
						<div class="inner">
							<h1>Footnotes</h1>
							<ul class="alt align-left">
								<li id="foot-1"><a href="#foot-1-ref" title="return">[1]</a><a href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/">https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/</a></li>
								<li id="foot-2"><a href="#foot-2-ref" title="return">[2]</a>Steve R Gunn, Support Vector Machines for Classification and Regression (1998)</li>
								<li id="foot-3"><a href="#foot-3-ref" title="return">[3]</a><a href = "http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf">http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf</a></li>
		<li id="foot-4"><a href="#foot-4-ref" title="return">[4]</a>Hari Babu Nandpuru, S. S. Salankar, V. R. Bora, MRI brain cancer classification using Support Vector Machine (2014)</li>
		<li id="foot-5"><a href="#foot-5-ref" title="return">[5]</a><a href = "https://uk.mathworks.com/help/images/texture-analysis-using-the-gray-level-co-occurrence-matrix-glcm.html">https://uk.mathworks.com/help/images/texture-analysis-using-the-gray-level-co-occurrence-matrix-glcm.html<a/></li>
		<li id="foot-6"><a href="#foot-6-ref" title="return">[6]</a>Vijay Wasule, Poonam Sonar, Classification of brain MRI using SVM and KNN classifier (2017)</li>
		<li id="foot-7"><a href="#foot-7-ref" title="return">[7]</a>Ahmed Kharrat, Mohamed Ben Halima, Mounir Ben Ayed, MRI brain tumor classification using Support Vector Machines and meta-heuristic method(2015)</li>
		<li id="foot-8"><a href="#foot-8-ref" title="return">[8]</a>Ketan Machhale, Hari Babu Nandpuru, Vivek Kapur, MRI brain cancer classification using hybrid classifier (SVM-KNN) (2015)</li>
		<li id="foot-9"><a href="#foot-9-ref" title="return">[9]</a><a href = "http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html">http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html</a></li>
		<li id="foot-10"><a href="#foot-10-ref" title="return">[10]</a><a href = "https://en.wikipedia.org/wiki/Polynomial_kernel">https://en.wikipedia.org/wiki/Polynomial_kernel</a></li>
		<li id="foot-11"><a href="#foot-11-ref" title="return">[11]</a><a href = "https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/">https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/</a></li>
		<li id="foot-12"><a href="#foot-12-ref" title="return">[12]</a><a href = "https://www.quora.com/What-are-the-advantages-of-support-vector-machines-SVM-compared-with-linear-regression-or-logistic-regression">https://www.quora.com/What-are-the-advantages-of-support-vector-machines-SVM-compared-with-linear-regression-or-logistic-regression</a></li>
		<li id="foot-13"><a href="#foot-13-ref" title="return">[13]</a><a href = "https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/">https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/</a></li>
		<li id="foot-14"><a href="#foot-14-ref" title="return">[14]</a><a href = "https://en.wikipedia.org/wiki/Radial_basis_function_kernel">https://en.wikipedia.org/wiki/Radial_basis_function_kernel</a></li>
		<li id="foot-15"><a href="#foot-15-ref" title="return">[15]</a><a href = "http://www.saedsayad.com/support_vector_machine.htm">http://www.saedsayad.com/support_vector_machine.htm</a></li>
		<li id="foot-16"><a href="#foot-16-ref" title="return">[16]</a><a href = "https://en.wikipedia.org/wiki/Support_vector_machine">https://en.wikipedia.org/wiki/Support_vector_machine</a></li>
		<li id="foot-17"><a href="#foot-17-ref" title="return">[17]</a><a href = "http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf">http://www.robots.ox.ac.uk/~az/lectures/ml/lect3.pdf</a></li>
							</ul>
						</div>
					</section>
	    </div>

	  <!-- Scripts -->
	    <script src="assets/js/jquery.min.js"></script>
	    <script src="assets/js/jquery.scrollex.min.js"></script>
	    <script src="assets/js/jquery.scrolly.min.js"></script>
	    <script src="assets/js/skel.min.js"></script>
	    <script src="assets/js/util.js"></script>
	    <script src="assets/js/main.js"></script>

	</body>
</html>
